{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Outline\n",
    "* Fit a neural net to the sberbank housing data\n",
    "* Clean and process the data\n",
    "* Attempted fits: shallow network, deep network, PCA\n",
    "* Hyperparameters: Number of nodes, number of layers, activation functions, batch normalization, weight initializers, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Clean...\n",
      "Feature Engineering...\n",
      "Rate Mults...\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "prediction_stderr = 0.0073  #  assumed standard error of predictions\n",
    "                          #  (smaller values make output closer to input)\n",
    "train_test_logmean_diff = 0.1  # assumed shift used to adjust frequencies for time trend\n",
    "probthresh = 90  # minimum probability*frequency to use new price instead of just rounding\n",
    "rounder = 2  # number of places left of decimal point to zero\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "    \n",
    "#load files\n",
    "train = pd.read_csv('train/train.csv', parse_dates=['timestamp'])\n",
    "test = pd.read_csv('test/test.csv', parse_dates=['timestamp'])\n",
    "id_test = test.id\n",
    "#df_test = pd.read_csv('test/test.csv')\n",
    "\n",
    "#clean data\n",
    "print('Data Clean...')\n",
    "bad_index = train[train.life_sq > train.full_sq].index\n",
    "train.loc[bad_index, \"life_sq\"] = np.NaN\n",
    "equal_index = [601,1896,2791]\n",
    "test.loc[equal_index, \"life_sq\"] = test.loc[equal_index, \"full_sq\"]\n",
    "bad_index = test[test.life_sq > test.full_sq].index\n",
    "test.loc[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = train[train.life_sq < 5].index\n",
    "train.loc[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = test[test.life_sq < 5].index\n",
    "test.loc[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = train[train.full_sq < 5].index\n",
    "train.loc[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = test[test.full_sq < 5].index\n",
    "test.loc[bad_index, \"full_sq\"] = np.NaN\n",
    "kitch_is_build_year = [13117]\n",
    "train.loc[kitch_is_build_year, \"build_year\"] = train.loc[kitch_is_build_year, \"kitch_sq\"]\n",
    "bad_index = train[train.kitch_sq >= train.life_sq].index\n",
    "train.loc[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = test[test.kitch_sq >= test.life_sq].index\n",
    "test.loc[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\n",
    "train.loc[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\n",
    "test.loc[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\n",
    "train.loc[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\n",
    "test.loc[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = train[train.life_sq > 300].index\n",
    "train.loc[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "bad_index = test[test.life_sq > 200].index\n",
    "test.loc[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "train.product_type.value_counts(normalize= True)\n",
    "test.product_type.value_counts(normalize= True)\n",
    "bad_index = train[train.build_year < 1500].index\n",
    "train.loc[bad_index, \"build_year\"] = np.NaN\n",
    "bad_index = test[test.build_year < 1500].index\n",
    "test.loc[bad_index, \"build_year\"] = np.NaN\n",
    "bad_index = train[train.num_room == 0].index\n",
    "train.loc[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = test[test.num_room == 0].index\n",
    "test.loc[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\n",
    "train.loc[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = [3174, 7313]\n",
    "test.loc[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\n",
    "train.loc[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\n",
    "bad_index = train[train.floor == 0].index\n",
    "train.loc[bad_index, \"floor\"] = np.NaN\n",
    "bad_index = train[train.max_floor == 0].index\n",
    "train.loc[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = test[test.max_floor == 0].index\n",
    "test.loc[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = train[train.floor > train.max_floor].index\n",
    "train.loc[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = test[test.floor > test.max_floor].index\n",
    "test.loc[bad_index, \"max_floor\"] = np.NaN\n",
    "train.floor.describe(percentiles= [0.9999])\n",
    "bad_index = [23584]\n",
    "train.loc[bad_index, \"floor\"] = np.NaN\n",
    "train.material.value_counts()\n",
    "test.material.value_counts()\n",
    "train.state.value_counts()\n",
    "bad_index = train[train.state == 33].index\n",
    "train.loc[bad_index, \"state\"] = np.NaN\n",
    "test.state.value_counts()\n",
    "\n",
    "# brings error down a lot by removing extreme price per sqm\n",
    "train.loc[train.full_sq == 0, 'full_sq'] = 50\n",
    "train = train[train.price_doc/train.full_sq <= 600000]\n",
    "train = train[train.price_doc/train.full_sq >= 10000]\n",
    "\n",
    "print('Feature Engineering...')\n",
    "# Add month-year\n",
    "month_year = (train.timestamp.dt.month*30 + train.timestamp.dt.year * 365)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "month_year = (test.timestamp.dt.month*30 + test.timestamp.dt.year * 365)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "test['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (train.timestamp.dt.weekofyear*7 + train.timestamp.dt.year * 365)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "week_year = (test.timestamp.dt.weekofyear*7 + test.timestamp.dt.year * 365)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "test['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "# Add month and day-of-week\n",
    "train['month'] = train.timestamp.dt.month\n",
    "train['dow'] = train.timestamp.dt.dayofweek\n",
    "\n",
    "test['month'] = test.timestamp.dt.month\n",
    "test['dow'] = test.timestamp.dt.dayofweek\n",
    "\n",
    "# Other feature engineering\n",
    "train['rel_floor'] = 0.05+train['floor'] / train['max_floor'].astype(float)\n",
    "train['rel_kitch_sq'] = 0.05+train['kitch_sq'] / train['full_sq'].astype(float)\n",
    "\n",
    "test['rel_floor'] = 0.05+test['floor'] / test['max_floor'].astype(float)\n",
    "test['rel_kitch_sq'] = 0.05+test['kitch_sq'] / test['full_sq'].astype(float)\n",
    "\n",
    "train.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\n",
    "test.apartment_name=test.sub_area + train['metro_km_avto'].astype(str)\n",
    "\n",
    "train['room_size'] = train['life_sq'] / train['num_room'].astype(float)\n",
    "test['room_size'] = test['life_sq'] / test['num_room'].astype(float)\n",
    "\n",
    "train['area_per_room'] = train['life_sq'] / train['num_room'].astype(float) #rough area per room\n",
    "train['livArea_ratio'] = train['life_sq'] / train['full_sq'].astype(float) #rough living area\n",
    "train['yrs_old'] = 2017 - train['build_year'].astype(float) #years old from 2017\n",
    "train['avgfloor_sq'] = train['life_sq']/train['max_floor'].astype(float) #living area per floor\n",
    "train['pts_floor_ratio'] = train['public_transport_station_km']/train['max_floor'].astype(float)\n",
    "# looking for significance of apartment buildings near public t \n",
    "train['room_size'] = train['life_sq'] / train['num_room'].astype(float)\n",
    "# doubled a var by accident\n",
    "# when removing one score did not improve...\n",
    "train['gender_ratio'] = train['male_f']/train['female_f'].astype(float)\n",
    "train['kg_park_ratio'] = train['kindergarten_km']/train['park_km'].astype(float) #significance of children?\n",
    "train['high_ed_extent'] = train['school_km'] / train['kindergarten_km'] #schooling\n",
    "train['pts_x_state'] = train['public_transport_station_km'] * train['state'].astype(float) #public trans * state of listing\n",
    "train['lifesq_x_state'] = train['life_sq'] * train['state'].astype(float) #life_sq times the state of the place\n",
    "train['floor_x_state'] = train['floor'] * train['state'].astype(float) #relative floor * the state of the place\n",
    "\n",
    "test['area_per_room'] = test['life_sq'] / test['num_room'].astype(float)\n",
    "test['livArea_ratio'] = test['life_sq'] / test['full_sq'].astype(float)\n",
    "test['yrs_old'] = 2017 - test['build_year'].astype(float)\n",
    "test['avgfloor_sq'] = test['life_sq']/test['max_floor'].astype(float) #living area per floor\n",
    "test['pts_floor_ratio'] = test['public_transport_station_km']/test['max_floor'].astype(float) #apartments near public t?\n",
    "test['room_size'] = test['life_sq'] / test['num_room'].astype(float)\n",
    "test['gender_ratio'] = test['male_f']/test['female_f'].astype(float)\n",
    "test['kg_park_ratio'] = test['kindergarten_km']/test['park_km'].astype(float)\n",
    "test['high_ed_extent'] = test['school_km'] / test['kindergarten_km']\n",
    "test['pts_x_state'] = test['public_transport_station_km'] * test['state'].astype(float) #public trans * state of listing\n",
    "test['lifesq_x_state'] = test['life_sq'] * test['state'].astype(float)\n",
    "test['floor_x_state'] = test['floor'] * test['state'].astype(float)\n",
    "\n",
    "#########################################################################\n",
    "print('Rate Mults...')\n",
    "# Aggreagte house price data derived from \n",
    "# http://www.globalpropertyguide.com/real-estate-house-prices/R#russia\n",
    "# by luckyzhou\n",
    "# See https://www.kaggle.com/luckyzhou/lzhou-test/comments\n",
    "\n",
    "rate_2015_q2 = 1\n",
    "rate_2015_q1 = rate_2015_q2 / 0.9932\n",
    "rate_2014_q4 = rate_2015_q1 / 1.0112\n",
    "rate_2014_q3 = rate_2014_q4 / 1.0169\n",
    "rate_2014_q2 = rate_2014_q3 / 1.0086\n",
    "rate_2014_q1 = rate_2014_q2 / 1.0126\n",
    "rate_2013_q4 = rate_2014_q1 / 0.9902\n",
    "rate_2013_q3 = rate_2013_q4 / 1.0041\n",
    "rate_2013_q2 = rate_2013_q3 / 1.0044\n",
    "rate_2013_q1 = rate_2013_q2 / 1.0104  # This is 1.002 (relative to mult), close to 1:\n",
    "rate_2012_q4 = rate_2013_q1 / 0.9832  #     maybe use 2013q1 as a base quarter and get rid of mult?\n",
    "rate_2012_q3 = rate_2012_q4 / 1.0277\n",
    "rate_2012_q2 = rate_2012_q3 / 1.0279\n",
    "rate_2012_q1 = rate_2012_q2 / 1.0279\n",
    "rate_2011_q4 = rate_2012_q1 / 1.076\n",
    "rate_2011_q3 = rate_2011_q4 / 1.0236\n",
    "rate_2011_q2 = rate_2011_q3 / 1\n",
    "rate_2011_q1 = rate_2011_q2 / 1.011\n",
    "\n",
    "\n",
    "# train 2015\n",
    "train['average_q_price'] = 1\n",
    "\n",
    "train_2015_q2_index = train.loc[train['timestamp'].dt.year == 2015].loc[train['timestamp'].dt.month >= 4].loc[train['timestamp'].dt.month < 7].index\n",
    "train.loc[train_2015_q2_index, 'average_q_price'] = rate_2015_q2\n",
    "\n",
    "train_2015_q1_index = train.loc[train['timestamp'].dt.year == 2015].loc[train['timestamp'].dt.month >= 1].loc[train['timestamp'].dt.month < 4].index\n",
    "train.loc[train_2015_q1_index, 'average_q_price'] = rate_2015_q1\n",
    "\n",
    "\n",
    "# train 2014\n",
    "train_2014_q4_index = train.loc[train['timestamp'].dt.year == 2014].loc[train['timestamp'].dt.month >= 10].loc[train['timestamp'].dt.month <= 12].index\n",
    "train.loc[train_2014_q4_index, 'average_q_price'] = rate_2014_q4\n",
    "\n",
    "train_2014_q3_index = train.loc[train['timestamp'].dt.year == 2014].loc[train['timestamp'].dt.month >= 7].loc[train['timestamp'].dt.month < 10].index\n",
    "train.loc[train_2014_q3_index, 'average_q_price'] = rate_2014_q3\n",
    "\n",
    "train_2014_q2_index = train.loc[train['timestamp'].dt.year == 2014].loc[train['timestamp'].dt.month >= 4].loc[train['timestamp'].dt.month < 7].index\n",
    "train.loc[train_2014_q2_index, 'average_q_price'] = rate_2014_q2\n",
    "\n",
    "train_2014_q1_index = train.loc[train['timestamp'].dt.year == 2014].loc[train['timestamp'].dt.month >= 1].loc[train['timestamp'].dt.month < 4].index\n",
    "train.loc[train_2014_q1_index, 'average_q_price'] = rate_2014_q1\n",
    "\n",
    "\n",
    "# train 2013\n",
    "train_2013_q4_index = train.loc[train['timestamp'].dt.year == 2013].loc[train['timestamp'].dt.month >= 10].loc[train['timestamp'].dt.month <= 12].index\n",
    "train.loc[train_2013_q4_index, 'average_q_price'] = rate_2013_q4\n",
    "\n",
    "train_2013_q3_index = train.loc[train['timestamp'].dt.year == 2013].loc[train['timestamp'].dt.month >= 7].loc[train['timestamp'].dt.month < 10].index\n",
    "train.loc[train_2013_q3_index, 'average_q_price'] = rate_2013_q3\n",
    "\n",
    "train_2013_q2_index = train.loc[train['timestamp'].dt.year == 2013].loc[train['timestamp'].dt.month >= 4].loc[train['timestamp'].dt.month < 7].index\n",
    "train.loc[train_2013_q2_index, 'average_q_price'] = rate_2013_q2\n",
    "\n",
    "train_2013_q1_index = train.loc[train['timestamp'].dt.year == 2013].loc[train['timestamp'].dt.month >= 1].loc[train['timestamp'].dt.month < 4].index\n",
    "train.loc[train_2013_q1_index, 'average_q_price'] = rate_2013_q1\n",
    "\n",
    "\n",
    "# train 2012\n",
    "train_2012_q4_index = train.loc[train['timestamp'].dt.year == 2012].loc[train['timestamp'].dt.month >= 10].loc[train['timestamp'].dt.month <= 12].index\n",
    "train.loc[train_2012_q4_index, 'average_q_price'] = rate_2012_q4\n",
    "\n",
    "train_2012_q3_index = train.loc[train['timestamp'].dt.year == 2012].loc[train['timestamp'].dt.month >= 7].loc[train['timestamp'].dt.month < 10].index\n",
    "train.loc[train_2012_q3_index, 'average_q_price'] = rate_2012_q3\n",
    "\n",
    "train_2012_q2_index = train.loc[train['timestamp'].dt.year == 2012].loc[train['timestamp'].dt.month >= 4].loc[train['timestamp'].dt.month < 7].index\n",
    "train.loc[train_2012_q2_index, 'average_q_price'] = rate_2012_q2\n",
    "\n",
    "train_2012_q1_index = train.loc[train['timestamp'].dt.year == 2012].loc[train['timestamp'].dt.month >= 1].loc[train['timestamp'].dt.month < 4].index\n",
    "train.loc[train_2012_q1_index, 'average_q_price'] = rate_2012_q1\n",
    "\n",
    "\n",
    "# train 2011\n",
    "train_2011_q4_index = train.loc[train['timestamp'].dt.year == 2011].loc[train['timestamp'].dt.month >= 10].loc[train['timestamp'].dt.month <= 12].index\n",
    "train.loc[train_2011_q4_index, 'average_q_price'] = rate_2011_q4\n",
    "\n",
    "train_2011_q3_index = train.loc[train['timestamp'].dt.year == 2011].loc[train['timestamp'].dt.month >= 7].loc[train['timestamp'].dt.month < 10].index\n",
    "train.loc[train_2011_q3_index, 'average_q_price'] = rate_2011_q3\n",
    "\n",
    "train_2011_q2_index = train.loc[train['timestamp'].dt.year == 2011].loc[train['timestamp'].dt.month >= 4].loc[train['timestamp'].dt.month < 7].index\n",
    "train.loc[train_2011_q2_index, 'average_q_price'] = rate_2011_q2\n",
    "\n",
    "train_2011_q1_index = train.loc[train['timestamp'].dt.year == 2011].loc[train['timestamp'].dt.month >= 1].loc[train['timestamp'].dt.month < 4].index\n",
    "train.loc[train_2011_q1_index, 'average_q_price'] = rate_2011_q1\n",
    "\n",
    "train['price_doc'] = train['price_doc'] * train['average_q_price']\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "mult = 1.054880504\n",
    "train['price_doc'] = train['price_doc'] * mult\n",
    "y_train = train[\"price_doc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values, one hot encode categorical values, normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\", \"average_q_price\"], axis=1)\n",
    "#x_test = test.drop([\"id\", \"timestamp\", \"average_q_price\"], axis=1)\n",
    "x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "\n",
    "num_train = len(x_train)\n",
    "x_all = pd.concat([x_train, x_test])\n",
    "\n",
    "for c in x_all.columns:\n",
    "    if x_all[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(x_all[c].values))\n",
    "        x_all[c] = lbl.transform(list(x_all[c].values))\n",
    "\n",
    "#indicator column for missing values and imputing the median for missing columns\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "def add_na_indicator_columns(df):\n",
    "    col_length = len(df)\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().values.any():\n",
    "            temp_series = pd.Series(np.zeros(col_length) )\n",
    "            temp_series[df[i].isnull().values] = 1\n",
    "            new_col_name = 'nan_bool_'+i\n",
    "            df[new_col_name] = pd.Series(temp_series,index=df.index)        \n",
    "\n",
    "add_na_indicator_columns(x_all)\n",
    "x_all = imp.fit_transform(x_all)\n",
    "\n",
    "#normalize the data\n",
    "X_scaler = StandardScaler()\n",
    "x_all = X_scaler.fit_transform(x_all)\n",
    "\n",
    "x_train = x_all[:num_train]\n",
    "x_test = x_all[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 38058\n",
      "Number of columns: 371\n",
      "Number of numeric columns: 371\n",
      "Number of columns with a NA value: 0\n"
     ]
    }
   ],
   "source": [
    "def describe_data_frame(df):\n",
    "    print(\"Number of rows: \" + str(len(df)))\n",
    "    print(\"Number of columns: \" + str(len(df.columns)))\n",
    "    \n",
    "    numeric_count = 0\n",
    "    na_count = 0\n",
    "    for i in df.columns:\n",
    "        if np.issubdtype(df[i].dtype, np.number):\n",
    "            numeric_count += 1\n",
    "        if df[i].isnull().values.any():\n",
    "            na_count += 1\n",
    "    print(\"Number of numeric columns: {}\".format(numeric_count))\n",
    "    print(\"Number of columns with a NA value: {}\".format(na_count))\n",
    "\n",
    "def describe_features(df,target_column,is_target_categorical):\n",
    "    for i in df.columns:\n",
    "        #print('\\n' + i + ' -- Feature ')\n",
    "        details = ''\n",
    "        \n",
    "        if df[i].isnull().values.any():\n",
    "            zFloat = len(df[df[i].isnull().values])/len(df[i]) \n",
    "            details += \" | NaN \" + str(np.round(zFloat,2))\n",
    "            print(i,details,len(df[df[i].isnull().values]))\n",
    "\n",
    "df_temp = pd.DataFrame(x_all)\n",
    "describe_data_frame(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_2, x_val, y_train_2, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Simple Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24316 samples, validate on 6080 samples\n",
      "Epoch 1/200\n",
      "24316/24316 [==============================] - 2s - loss: 94.9054 - val_loss: 62.6717\n",
      "Epoch 2/200\n",
      "24316/24316 [==============================] - 2s - loss: 52.3201 - val_loss: 43.8791\n",
      "Epoch 3/200\n",
      "24316/24316 [==============================] - 2s - loss: 38.9724 - val_loss: 34.3226\n",
      "Epoch 4/200\n",
      "24316/24316 [==============================] - 2s - loss: 31.3229 - val_loss: 28.2160\n",
      "Epoch 5/200\n",
      "24316/24316 [==============================] - 2s - loss: 26.1512 - val_loss: 23.8651\n",
      "Epoch 6/200\n",
      "24316/24316 [==============================] - 2s - loss: 22.3405 - val_loss: 20.5564\n",
      "Epoch 7/200\n",
      "24316/24316 [==============================] - 2s - loss: 19.3785 - val_loss: 17.9310\n",
      "Epoch 8/200\n",
      "24316/24316 [==============================] - 2s - loss: 16.9907 - val_loss: 15.7835\n",
      "Epoch 9/200\n",
      "24316/24316 [==============================] - 2s - loss: 15.0147 - val_loss: 13.9869\n",
      "Epoch 10/200\n",
      "24316/24316 [==============================] - 2s - loss: 13.3465 - val_loss: 12.4581\n",
      "Epoch 11/200\n",
      "24316/24316 [==============================] - 2s - loss: 11.9165 - val_loss: 11.1389\n",
      "Epoch 12/200\n",
      "24316/24316 [==============================] - 2s - loss: 10.6757 - val_loss: 9.9882\n",
      "Epoch 13/200\n",
      "24316/24316 [==============================] - 2s - loss: 9.5886 - val_loss: 8.9762\n",
      "Epoch 14/200\n",
      "24316/24316 [==============================] - 2s - loss: 8.6284 - val_loss: 8.0798\n",
      "Epoch 15/200\n",
      "24316/24316 [==============================] - 2s - loss: 7.7748 - val_loss: 7.2806\n",
      "Epoch 16/200\n",
      "24316/24316 [==============================] - 2s - loss: 7.0121 - val_loss: 6.5655\n",
      "Epoch 17/200\n",
      "24316/24316 [==============================] - 1s - loss: 6.3275 - val_loss: 5.9227\n",
      "Epoch 18/200\n",
      "24316/24316 [==============================] - 1s - loss: 5.7111 - val_loss: 5.3430\n",
      "Epoch 19/200\n",
      "24316/24316 [==============================] - 1s - loss: 5.1546 - val_loss: 4.8197\n",
      "Epoch 20/200\n",
      "24316/24316 [==============================] - 2s - loss: 4.6512 - val_loss: 4.3458\n",
      "Epoch 21/200\n",
      "24316/24316 [==============================] - 1s - loss: 4.1950 - val_loss: 3.9166\n",
      "Epoch 22/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.7813 - val_loss: 3.5272\n",
      "Epoch 23/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.4061 - val_loss: 3.1747\n",
      "Epoch 24/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.0656 - val_loss: 2.8548\n",
      "Epoch 25/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.7568 - val_loss: 2.5651\n",
      "Epoch 26/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.4770 - val_loss: 2.3029\n",
      "Epoch 27/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.2239 - val_loss: 2.0661\n",
      "Epoch 28/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.9952 - val_loss: 1.8526\n",
      "Epoch 29/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.7891 - val_loss: 1.6605\n",
      "Epoch 30/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.6037 - val_loss: 1.4884\n",
      "Epoch 31/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.4374 - val_loss: 1.3343\n",
      "Epoch 32/200\n",
      "24316/24316 [==============================] - 2s - loss: 1.2888 - val_loss: 1.1971\n",
      "Epoch 33/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.1565 - val_loss: 1.0752\n",
      "Epoch 34/200\n",
      "24316/24316 [==============================] - 2s - loss: 1.0391 - val_loss: 0.9676\n",
      "Epoch 35/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.9355 - val_loss: 0.8729\n",
      "Epoch 36/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.8443 - val_loss: 0.7901\n",
      "Epoch 37/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.7645 - val_loss: 0.7179\n",
      "Epoch 38/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.6952 - val_loss: 0.6554\n",
      "Epoch 39/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.6351 - val_loss: 0.6017\n",
      "Epoch 40/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5836 - val_loss: 0.5557\n",
      "Epoch 41/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5395 - val_loss: 0.5167\n",
      "Epoch 42/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.5021 - val_loss: 0.4836\n",
      "Epoch 43/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.4704 - val_loss: 0.4560\n",
      "Epoch 44/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.4438 - val_loss: 0.4328\n",
      "Epoch 45/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.4216 - val_loss: 0.4135\n",
      "Epoch 46/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.4029 - val_loss: 0.3972\n",
      "Epoch 47/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3873 - val_loss: 0.3835\n",
      "Epoch 48/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3741 - val_loss: 0.3721\n",
      "Epoch 49/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3630 - val_loss: 0.3621\n",
      "Epoch 50/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3533 - val_loss: 0.3535\n",
      "Epoch 51/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3450 - val_loss: 0.3459\n",
      "Epoch 52/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3376 - val_loss: 0.3391\n",
      "Epoch 53/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3310 - val_loss: 0.3329\n",
      "Epoch 54/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3250 - val_loss: 0.3272\n",
      "Epoch 55/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3194 - val_loss: 0.3217\n",
      "Epoch 56/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3141 - val_loss: 0.3166\n",
      "Epoch 57/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3091 - val_loss: 0.3115\n",
      "Epoch 58/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.3042 - val_loss: 0.3067\n",
      "Epoch 59/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2995 - val_loss: 0.3019\n",
      "Epoch 60/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2949 - val_loss: 0.2974\n",
      "Epoch 61/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2905 - val_loss: 0.2929\n",
      "Epoch 62/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2863 - val_loss: 0.2885\n",
      "Epoch 63/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2822 - val_loss: 0.2844\n",
      "Epoch 64/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2783 - val_loss: 0.2805\n",
      "Epoch 65/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2746 - val_loss: 0.2767\n",
      "Epoch 66/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2710 - val_loss: 0.2731\n",
      "Epoch 67/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2677 - val_loss: 0.2698\n",
      "Epoch 68/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2646 - val_loss: 0.2666\n",
      "Epoch 69/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2616 - val_loss: 0.2637\n",
      "Epoch 70/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2589 - val_loss: 0.2609\n",
      "Epoch 71/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2563 - val_loss: 0.2583\n",
      "Epoch 72/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2539 - val_loss: 0.2560\n",
      "Epoch 73/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2517 - val_loss: 0.2538\n",
      "Epoch 74/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2496 - val_loss: 0.2518\n",
      "Epoch 75/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2477 - val_loss: 0.2499\n",
      "Epoch 76/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2460 - val_loss: 0.2482\n",
      "Epoch 77/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2443 - val_loss: 0.2466\n",
      "Epoch 78/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2427 - val_loss: 0.2449\n",
      "Epoch 79/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2413 - val_loss: 0.2438\n",
      "Epoch 80/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2400 - val_loss: 0.2425\n",
      "Epoch 81/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2387 - val_loss: 0.2412\n",
      "Epoch 82/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2375 - val_loss: 0.2400\n",
      "Epoch 83/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2364 - val_loss: 0.2391\n",
      "Epoch 84/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2353 - val_loss: 0.2379\n",
      "Epoch 85/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2342 - val_loss: 0.2370\n",
      "Epoch 86/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2332 - val_loss: 0.2362\n",
      "Epoch 87/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2323 - val_loss: 0.2354\n",
      "Epoch 88/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2313 - val_loss: 0.2345\n",
      "Epoch 89/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2304 - val_loss: 0.2336\n",
      "Epoch 90/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2295 - val_loss: 0.2331\n",
      "Epoch 91/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2287 - val_loss: 0.2322\n",
      "Epoch 92/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2279 - val_loss: 0.2315\n",
      "Epoch 93/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2271 - val_loss: 0.2308\n",
      "Epoch 94/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2263 - val_loss: 0.2300\n",
      "Epoch 95/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2255 - val_loss: 0.2297\n",
      "Epoch 96/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2248 - val_loss: 0.2288\n",
      "Epoch 97/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2241 - val_loss: 0.2284\n",
      "Epoch 98/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2234 - val_loss: 0.2279\n",
      "Epoch 99/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2228 - val_loss: 0.2272\n",
      "Epoch 100/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2221 - val_loss: 0.2267\n",
      "Epoch 101/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2215 - val_loss: 0.2261\n",
      "Epoch 102/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2209 - val_loss: 0.2257\n",
      "Epoch 103/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2204 - val_loss: 0.2252\n",
      "Epoch 104/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2198 - val_loss: 0.2249\n",
      "Epoch 105/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2193 - val_loss: 0.2243\n",
      "Epoch 106/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2187 - val_loss: 0.2238\n",
      "Epoch 107/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2183 - val_loss: 0.2236\n",
      "Epoch 108/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2178 - val_loss: 0.2232\n",
      "Epoch 109/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2173 - val_loss: 0.2229\n",
      "Epoch 110/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2169 - val_loss: 0.2224\n",
      "Epoch 111/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2164 - val_loss: 0.2220\n",
      "Epoch 112/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2160 - val_loss: 0.2220\n",
      "Epoch 113/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2156 - val_loss: 0.2215\n",
      "Epoch 114/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2152 - val_loss: 0.2213\n",
      "Epoch 115/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2149 - val_loss: 0.2212\n",
      "Epoch 116/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2145 - val_loss: 0.2206\n",
      "Epoch 117/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2141 - val_loss: 0.2205\n",
      "Epoch 118/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2138 - val_loss: 0.2202\n",
      "Epoch 119/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2134 - val_loss: 0.2198\n",
      "Epoch 120/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2132 - val_loss: 0.2197\n",
      "Epoch 121/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2128 - val_loss: 0.2198\n",
      "Epoch 122/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2126 - val_loss: 0.2195\n",
      "Epoch 123/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2123 - val_loss: 0.2193\n",
      "Epoch 124/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2120 - val_loss: 0.2190\n",
      "Epoch 125/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2117 - val_loss: 0.2188\n",
      "Epoch 126/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2114 - val_loss: 0.2188\n",
      "Epoch 127/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2112 - val_loss: 0.2185\n",
      "Epoch 128/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2108 - val_loss: 0.2184\n",
      "Epoch 129/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2106 - val_loss: 0.2185\n",
      "Epoch 130/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2104 - val_loss: 0.2183\n",
      "Epoch 131/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2102 - val_loss: 0.2183\n",
      "Epoch 132/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2100 - val_loss: 0.2177\n",
      "Epoch 133/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2097 - val_loss: 0.2177\n",
      "Epoch 134/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2095 - val_loss: 0.2179\n",
      "Epoch 135/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2093 - val_loss: 0.2174\n",
      "Epoch 136/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2090 - val_loss: 0.2173\n",
      "Epoch 137/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2088 - val_loss: 0.2171\n",
      "Epoch 138/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2086 - val_loss: 0.2173\n",
      "Epoch 139/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2084 - val_loss: 0.2170\n",
      "Epoch 140/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2082 - val_loss: 0.2170\n",
      "Epoch 141/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2080 - val_loss: 0.2171\n",
      "Epoch 142/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2079 - val_loss: 0.2168\n",
      "Epoch 143/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2076 - val_loss: 0.2168\n",
      "Epoch 144/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2074 - val_loss: 0.2164\n",
      "Epoch 145/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2073 - val_loss: 0.2165\n",
      "Epoch 146/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2071 - val_loss: 0.2164\n",
      "Epoch 147/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2069 - val_loss: 0.2166\n",
      "Epoch 148/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2068 - val_loss: 0.2164\n",
      "Epoch 149/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2065 - val_loss: 0.2164\n",
      "Epoch 150/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2064 - val_loss: 0.2164\n",
      "Epoch 151/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2062 - val_loss: 0.2164\n",
      "Epoch 152/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2060 - val_loss: 0.2165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbd1a20>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "epsilon = 10e-9\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.log(K.clip(y_pred,epsilon,np.inf)+1.0) - \n",
    "                                  K.log(K.clip(y_true,epsilon,np.inf)+1.0)), axis=-1)) \n",
    "\n",
    "# def root_mean_squared_error(y_true, y_pred):\n",
    "#         return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "#adam_2 = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipnorm=1.)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(450,input_dim=(371),activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(225,activation='relu'))\n",
    "model.add(Dense(1,activation='relu'))\n",
    "#model.compile(loss=rmsle,optimizer='adam')\n",
    "model.compile(loss='mean_squared_logarithmic_error',optimizer='adam')\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "    #callbacks.\n",
    "    ModelCheckpoint('keras_checkpoints/simple', monitor='val_loss', verbose=0, \n",
    "                    save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "]\n",
    "\n",
    "model.fit(x_train_2,y_train_2,epochs=200,batch_size=128,validation_data=(x_val,y_val),callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  price_doc\n",
      "0  30474  4298447.5\n",
      "1  30475  7388918.5\n",
      "2  30476  4074422.5\n",
      "3  30477  6374046.0\n",
      "4  30478  4172210.0\n"
     ]
    }
   ],
   "source": [
    "model_pred_1 = model.predict(x_test)\n",
    "\n",
    "def output_predictions(name,id_col,predictions):\n",
    "    df_out = pd.DataFrame({\"id\":id_col,\"price_doc\":predictions}) \n",
    "    print(df_out.head())\n",
    "    df_out.to_csv(name+\"_sberbank_submission.csv\",index=False)\n",
    "    \n",
    "output_predictions(\"keras_2_simple\",id_test,model_pred_1[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24999216  0.12657392  0.0600045   0.05633772  0.03547774  0.02812155\n",
      "  0.02344738  0.02032039  0.01883119  0.0178446   0.01572641  0.01355604\n",
      "  0.01346917  0.01111229  0.01076332  0.01057343  0.00991622  0.00895608\n",
      "  0.00832012  0.0082516   0.00729814  0.00673189  0.00651013  0.00630501\n",
      "  0.00605212  0.00594904  0.00554553  0.00540598  0.00540158  0.00513978\n",
      "  0.00481644  0.00460194  0.00447586  0.00430218  0.00421243  0.00406572\n",
      "  0.00400286  0.00394177  0.0039007   0.00357575  0.00351459  0.00344585\n",
      "  0.00336151  0.00329017  0.00316343  0.00304065  0.00292467  0.00283409\n",
      "  0.00272798  0.00270844  0.00270373  0.00264536  0.00263292  0.00259292\n",
      "  0.00255301  0.00251074  0.00246334  0.00235041  0.00231534  0.00220009\n",
      "  0.00216769  0.00216024  0.00211754  0.00207456]\n",
      "0.914329965393\n"
     ]
    }
   ],
   "source": [
    "#keras model 2\n",
    "#using 64 component pca\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 64,random_state=0)\n",
    "pca_all = pca.fit_transform(x_all)\n",
    "pca_x_train = pca_all[:num_train]\n",
    "pca_x_test = pca_all[num_train:]\n",
    "\n",
    "pca_x_train_2, pca_x_val, pca_y_train_2, pca_y_val = train_test_split(pca_x_train, y_train, test_size=0.2, random_state=31)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24316 samples, validate on 6080 samples\n",
      "Epoch 1/200\n",
      "24316/24316 [==============================] - 2s - loss: 167.6350 - val_loss: 145.3646\n",
      "Epoch 2/200\n",
      "24316/24316 [==============================] - 1s - loss: 137.1393 - val_loss: 131.8172\n",
      "Epoch 3/200\n",
      "24316/24316 [==============================] - 1s - loss: 125.7773 - val_loss: 124.0907\n",
      "Epoch 4/200\n",
      "24316/24316 [==============================] - 1s - loss: 118.5915 - val_loss: 118.4270\n",
      "Epoch 5/200\n",
      "24316/24316 [==============================] - 1s - loss: 113.3340 - val_loss: 114.2079\n",
      "Epoch 6/200\n",
      "24316/24316 [==============================] - 1s - loss: 105.1235 - val_loss: 98.1970\n",
      "Epoch 7/200\n",
      "24316/24316 [==============================] - 1s - loss: 82.0816 - val_loss: 69.2943\n",
      "Epoch 8/200\n",
      "24316/24316 [==============================] - 1s - loss: 65.7486 - val_loss: 55.7928\n",
      "Epoch 9/200\n",
      "24316/24316 [==============================] - 1s - loss: 55.3314 - val_loss: 52.3214\n",
      "Epoch 10/200\n",
      "24316/24316 [==============================] - 1s - loss: 51.4598 - val_loss: 48.9370\n",
      "Epoch 11/200\n",
      "24316/24316 [==============================] - 1s - loss: 47.9124 - val_loss: 46.0428\n",
      "Epoch 12/200\n",
      "24316/24316 [==============================] - 1s - loss: 45.2299 - val_loss: 43.2038\n",
      "Epoch 13/200\n",
      "24316/24316 [==============================] - 1s - loss: 42.7649 - val_loss: 41.1471\n",
      "Epoch 14/200\n",
      "24316/24316 [==============================] - 1s - loss: 40.7204 - val_loss: 39.2871\n",
      "Epoch 15/200\n",
      "24316/24316 [==============================] - 1s - loss: 39.0070 - val_loss: 36.9727\n",
      "Epoch 16/200\n",
      "24316/24316 [==============================] - 1s - loss: 37.2051 - val_loss: 35.7692\n",
      "Epoch 17/200\n",
      "24316/24316 [==============================] - 1s - loss: 35.5416 - val_loss: 34.3658\n",
      "Epoch 18/200\n",
      "24316/24316 [==============================] - 1s - loss: 34.0428 - val_loss: 33.0511\n",
      "Epoch 19/200\n",
      "24316/24316 [==============================] - 1s - loss: 32.7526 - val_loss: 31.7072\n",
      "Epoch 20/200\n",
      "24316/24316 [==============================] - 1s - loss: 31.3674 - val_loss: 30.6411\n",
      "Epoch 21/200\n",
      "24316/24316 [==============================] - 1s - loss: 29.0587 - val_loss: 25.7332\n",
      "Epoch 22/200\n",
      "24316/24316 [==============================] - 1s - loss: 25.8875 - val_loss: 23.3416\n",
      "Epoch 23/200\n",
      "24316/24316 [==============================] - 1s - loss: 22.3767 - val_loss: 21.3184\n",
      "Epoch 24/200\n",
      "24316/24316 [==============================] - 1s - loss: 20.6422 - val_loss: 19.9711\n",
      "Epoch 25/200\n",
      "24316/24316 [==============================] - 1s - loss: 19.5459 - val_loss: 18.9166\n",
      "Epoch 26/200\n",
      "24316/24316 [==============================] - 1s - loss: 18.5428 - val_loss: 17.9565\n",
      "Epoch 27/200\n",
      "24316/24316 [==============================] - 1s - loss: 17.6073 - val_loss: 17.0684\n",
      "Epoch 28/200\n",
      "24316/24316 [==============================] - 1s - loss: 16.7272 - val_loss: 16.2150\n",
      "Epoch 29/200\n",
      "24316/24316 [==============================] - 1s - loss: 15.8949 - val_loss: 15.4279\n",
      "Epoch 30/200\n",
      "24316/24316 [==============================] - 1s - loss: 15.1146 - val_loss: 14.6565\n",
      "Epoch 31/200\n",
      "24316/24316 [==============================] - 1s - loss: 14.3566 - val_loss: 13.9294\n",
      "Epoch 32/200\n",
      "24316/24316 [==============================] - 1s - loss: 13.6456 - val_loss: 13.2627\n",
      "Epoch 33/200\n",
      "24316/24316 [==============================] - 1s - loss: 12.9616 - val_loss: 12.5938\n",
      "Epoch 34/200\n",
      "24316/24316 [==============================] - 1s - loss: 12.3118 - val_loss: 11.9650\n",
      "Epoch 35/200\n",
      "24316/24316 [==============================] - 1s - loss: 11.6908 - val_loss: 11.3612\n",
      "Epoch 36/200\n",
      "24316/24316 [==============================] - 1s - loss: 11.0985 - val_loss: 10.7933\n",
      "Epoch 37/200\n",
      "24316/24316 [==============================] - 1s - loss: 10.5314 - val_loss: 10.2487\n",
      "Epoch 38/200\n",
      "24316/24316 [==============================] - 1s - loss: 9.9879 - val_loss: 9.7129\n",
      "Epoch 39/200\n",
      "24316/24316 [==============================] - 1s - loss: 9.4689 - val_loss: 9.2041\n",
      "Epoch 40/200\n",
      "24316/24316 [==============================] - 1s - loss: 8.9735 - val_loss: 8.7237\n",
      "Epoch 41/200\n",
      "24316/24316 [==============================] - 1s - loss: 8.4991 - val_loss: 8.2640\n",
      "Epoch 42/200\n",
      "24316/24316 [==============================] - 1s - loss: 8.0467 - val_loss: 7.8304\n",
      "Epoch 43/200\n",
      "24316/24316 [==============================] - 1s - loss: 7.6140 - val_loss: 7.4077\n",
      "Epoch 44/200\n",
      "24316/24316 [==============================] - 1s - loss: 7.2002 - val_loss: 7.0113\n",
      "Epoch 45/200\n",
      "24316/24316 [==============================] - 1s - loss: 6.8051 - val_loss: 6.6307\n",
      "Epoch 46/200\n",
      "24316/24316 [==============================] - 1s - loss: 6.4288 - val_loss: 6.2608\n",
      "Epoch 47/200\n",
      "24316/24316 [==============================] - 1s - loss: 6.0687 - val_loss: 5.9137\n",
      "Epoch 48/200\n",
      "24316/24316 [==============================] - 1s - loss: 5.7256 - val_loss: 5.5882\n",
      "Epoch 49/200\n",
      "24316/24316 [==============================] - 1s - loss: 5.3989 - val_loss: 5.2619\n",
      "Epoch 50/200\n",
      "24316/24316 [==============================] - 1s - loss: 5.0875 - val_loss: 4.9646\n",
      "Epoch 51/200\n",
      "24316/24316 [==============================] - 1s - loss: 4.7916 - val_loss: 4.6846\n",
      "Epoch 52/200\n",
      "24316/24316 [==============================] - 1s - loss: 4.5097 - val_loss: 4.4080\n",
      "Epoch 53/200\n",
      "24316/24316 [==============================] - 1s - loss: 4.2413 - val_loss: 4.1512\n",
      "Epoch 54/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.9868 - val_loss: 3.9075\n",
      "Epoch 55/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.7452 - val_loss: 3.6668\n",
      "Epoch 56/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.5158 - val_loss: 3.4508\n",
      "Epoch 57/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.2988 - val_loss: 3.2388\n",
      "Epoch 58/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.0927 - val_loss: 3.0395\n",
      "Epoch 59/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.8982 - val_loss: 2.8546\n",
      "Epoch 60/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.7141 - val_loss: 2.6710\n",
      "Epoch 61/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.5414 - val_loss: 2.5130\n",
      "Epoch 62/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.3772 - val_loss: 2.3548\n",
      "Epoch 63/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.2240 - val_loss: 2.2048\n",
      "Epoch 64/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.0790 - val_loss: 2.0662\n",
      "Epoch 65/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.9429 - val_loss: 1.9346\n",
      "Epoch 66/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.8155 - val_loss: 1.8126\n",
      "Epoch 67/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.6961 - val_loss: 1.7015\n",
      "Epoch 68/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.5844 - val_loss: 1.5968\n",
      "Epoch 69/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.4799 - val_loss: 1.4907\n",
      "Epoch 70/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.3826 - val_loss: 1.3976\n",
      "Epoch 71/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.2919 - val_loss: 1.3149\n",
      "Epoch 72/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.2080 - val_loss: 1.2375\n",
      "Epoch 73/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.1297 - val_loss: 1.1597\n",
      "Epoch 74/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.0573 - val_loss: 1.0941\n",
      "Epoch 75/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.9905 - val_loss: 1.0373\n",
      "Epoch 76/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.9292 - val_loss: 0.9719\n",
      "Epoch 77/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.8726 - val_loss: 0.9240\n",
      "Epoch 78/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.8205 - val_loss: 0.8788\n",
      "Epoch 79/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.7731 - val_loss: 0.8333\n",
      "Epoch 80/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.7295 - val_loss: 0.7997\n",
      "Epoch 81/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.6898 - val_loss: 0.7617\n",
      "Epoch 82/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.6533 - val_loss: 0.7328\n",
      "Epoch 83/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.6208 - val_loss: 0.7035\n",
      "Epoch 84/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5921 - val_loss: 0.6736\n",
      "Epoch 85/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5657 - val_loss: 0.6480\n",
      "Epoch 86/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5421 - val_loss: 0.6326\n",
      "Epoch 87/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5210 - val_loss: 0.6221\n",
      "Epoch 88/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3541 - val_loss: 0.3027\n",
      "Epoch 89/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3059 - val_loss: 0.2884\n",
      "Epoch 90/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2920 - val_loss: 0.2823\n",
      "Epoch 91/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2801 - val_loss: 0.2739\n",
      "Epoch 92/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2705 - val_loss: 0.2679\n",
      "Epoch 93/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2627 - val_loss: 0.2590\n",
      "Epoch 94/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2545 - val_loss: 0.2517\n",
      "Epoch 95/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2490 - val_loss: 0.2545\n",
      "Epoch 96/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2435 - val_loss: 0.2469\n",
      "Epoch 97/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2388 - val_loss: 0.2371\n",
      "Epoch 98/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2353 - val_loss: 0.2438\n",
      "Epoch 99/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2317 - val_loss: 0.2387\n",
      "Epoch 100/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2293 - val_loss: 0.2354\n",
      "Epoch 101/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2276 - val_loss: 0.2305\n",
      "Epoch 102/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2244 - val_loss: 0.2303\n",
      "Epoch 103/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2228 - val_loss: 0.2322\n",
      "Epoch 104/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2107 - val_loss: 0.2299\n",
      "Epoch 105/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2096 - val_loss: 0.2299\n",
      "Epoch 106/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2079 - val_loss: 0.2287\n",
      "Epoch 107/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2068 - val_loss: 0.2296\n",
      "Epoch 108/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2059 - val_loss: 0.2280\n",
      "Epoch 109/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2044 - val_loss: 0.2286\n",
      "Epoch 110/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2033 - val_loss: 0.2312\n",
      "Epoch 111/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2025 - val_loss: 0.2275\n",
      "Epoch 112/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2097 - val_loss: 0.2294\n",
      "Epoch 113/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2006 - val_loss: 0.2308\n",
      "Epoch 114/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2016 - val_loss: 0.2275\n",
      "Epoch 115/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.1998 - val_loss: 4.6233\n",
      "Epoch 116/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2314 - val_loss: 0.2316\n",
      "Epoch 117/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2124 - val_loss: 0.2268\n",
      "Epoch 118/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2066 - val_loss: 0.2251\n",
      "Epoch 119/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2022 - val_loss: 0.2261\n",
      "Epoch 120/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2088 - val_loss: 0.2419\n",
      "Epoch 121/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2123 - val_loss: 0.2274\n",
      "Epoch 122/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2053 - val_loss: 0.2255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1729aeb8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(128,input_dim=(64),activation='elu'))\n",
    "model_2.add(BatchNormalization())\n",
    "#model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(128,input_dim=(64),activation='elu'))\n",
    "model_2.add(BatchNormalization())\n",
    "#model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(128,activation='elu'))\n",
    "model_2.add(Dense(1,activation='linear'))\n",
    "#model_2.compile(loss=rmsle,optimizer='adam')\n",
    "model_2.compile(loss='mean_squared_logarithmic_error',optimizer='adam')\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "    #callbacks.\n",
    "    ModelCheckpoint('keras_checkpoints/model_2_bn', monitor='val_loss', verbose=0, \n",
    "                    save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "]\n",
    "\n",
    "model_2.fit(pca_x_train_2,pca_y_train_2,epochs=200,batch_size=128,validation_data=(pca_x_val,pca_y_val),callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  price_doc\n",
      "0  30474  3612644.0\n",
      "1  30475  8047797.0\n",
      "2  30476  4315007.0\n",
      "3  30477  4484490.0\n",
      "4  30478  3340141.5\n"
     ]
    }
   ],
   "source": [
    "model_pred_2 = model_2.predict(pca_x_test)\n",
    "\n",
    "output_predictions(\"keras_2_model_2_bn\",id_test,model_pred_2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24316 samples, validate on 6080 samples\n",
      "Epoch 1/200\n",
      "24316/24316 [==============================] - 1s - loss: 86.8942 - val_loss: 50.1234\n",
      "Epoch 2/200\n",
      "24316/24316 [==============================] - 1s - loss: 38.8354 - val_loss: 29.8341\n",
      "Epoch 3/200\n",
      "24316/24316 [==============================] - 1s - loss: 24.9384 - val_loss: 20.3452\n",
      "Epoch 4/200\n",
      "24316/24316 [==============================] - 1s - loss: 17.6062 - val_loss: 14.7616\n",
      "Epoch 5/200\n",
      "24316/24316 [==============================] - 1s - loss: 13.0285 - val_loss: 11.0922\n",
      "Epoch 6/200\n",
      "24316/24316 [==============================] - 1s - loss: 9.9319 - val_loss: 8.5214\n",
      "Epoch 7/200\n",
      "24316/24316 [==============================] - 1s - loss: 7.7017 - val_loss: 6.6455\n",
      "Epoch 8/200\n",
      "24316/24316 [==============================] - 1s - loss: 6.0490 - val_loss: 5.2370\n",
      "Epoch 9/200\n",
      "24316/24316 [==============================] - 1s - loss: 4.8129 - val_loss: 4.1565\n",
      "Epoch 10/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.8442 - val_loss: 3.3184\n",
      "Epoch 11/200\n",
      "24316/24316 [==============================] - 1s - loss: 3.0903 - val_loss: 2.6609\n",
      "Epoch 12/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.4943 - val_loss: 2.1423\n",
      "Epoch 13/200\n",
      "24316/24316 [==============================] - 1s - loss: 2.0173 - val_loss: 1.7332\n",
      "Epoch 14/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.6462 - val_loss: 1.4092\n",
      "Epoch 15/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.3488 - val_loss: 1.1530\n",
      "Epoch 16/200\n",
      "24316/24316 [==============================] - 1s - loss: 1.1101 - val_loss: 0.9521\n",
      "Epoch 17/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.9268 - val_loss: 0.7947\n",
      "Epoch 18/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.7851 - val_loss: 0.6718\n",
      "Epoch 19/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.6662 - val_loss: 0.5776\n",
      "Epoch 20/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5816 - val_loss: 0.5053\n",
      "Epoch 21/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.5177 - val_loss: 0.4502\n",
      "Epoch 22/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.4637 - val_loss: 0.4092\n",
      "Epoch 23/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.4215 - val_loss: 0.3784\n",
      "Epoch 24/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3964 - val_loss: 0.3553\n",
      "Epoch 25/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3733 - val_loss: 0.3378\n",
      "Epoch 26/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3575 - val_loss: 0.3245\n",
      "Epoch 27/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3438 - val_loss: 0.3141\n",
      "Epoch 28/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3335 - val_loss: 0.3057\n",
      "Epoch 29/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3255 - val_loss: 0.2986\n",
      "Epoch 30/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3164 - val_loss: 0.2923\n",
      "Epoch 31/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3128 - val_loss: 0.2868\n",
      "Epoch 32/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3074 - val_loss: 0.2818\n",
      "Epoch 33/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.3008 - val_loss: 0.2773\n",
      "Epoch 34/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2954 - val_loss: 0.2731\n",
      "Epoch 35/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2926 - val_loss: 0.2694\n",
      "Epoch 36/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2866 - val_loss: 0.2660\n",
      "Epoch 37/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2866 - val_loss: 0.2630\n",
      "Epoch 38/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2819 - val_loss: 0.2604\n",
      "Epoch 39/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2800 - val_loss: 0.2581\n",
      "Epoch 40/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2800 - val_loss: 0.2560\n",
      "Epoch 41/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2764 - val_loss: 0.2540\n",
      "Epoch 42/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2735 - val_loss: 0.2524\n",
      "Epoch 43/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2723 - val_loss: 0.2509\n",
      "Epoch 44/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2696 - val_loss: 0.2496\n",
      "Epoch 45/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2692 - val_loss: 0.2484\n",
      "Epoch 46/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2661 - val_loss: 0.2473\n",
      "Epoch 47/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2666 - val_loss: 0.2462\n",
      "Epoch 48/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2643 - val_loss: 0.2453\n",
      "Epoch 49/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2643 - val_loss: 0.2444\n",
      "Epoch 50/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2635 - val_loss: 0.2436\n",
      "Epoch 51/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2614 - val_loss: 0.2430\n",
      "Epoch 52/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2595 - val_loss: 0.2422\n",
      "Epoch 53/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2610 - val_loss: 0.2416\n",
      "Epoch 54/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2597 - val_loss: 0.2409\n",
      "Epoch 55/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2575 - val_loss: 0.2401\n",
      "Epoch 56/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2559 - val_loss: 0.2397\n",
      "Epoch 57/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2570 - val_loss: 0.2390\n",
      "Epoch 58/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2557 - val_loss: 0.2386\n",
      "Epoch 59/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2534 - val_loss: 0.2379\n",
      "Epoch 60/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2537 - val_loss: 0.2374\n",
      "Epoch 61/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2533 - val_loss: 0.2373\n",
      "Epoch 62/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2527 - val_loss: 0.2367\n",
      "Epoch 63/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2512 - val_loss: 0.2362\n",
      "Epoch 64/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2519 - val_loss: 0.2360\n",
      "Epoch 65/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2512 - val_loss: 0.2357\n",
      "Epoch 66/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2495 - val_loss: 0.2350\n",
      "Epoch 67/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2474 - val_loss: 0.2346\n",
      "Epoch 68/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2492 - val_loss: 0.2346\n",
      "Epoch 69/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2485 - val_loss: 0.2342\n",
      "Epoch 70/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2453 - val_loss: 0.2339\n",
      "Epoch 71/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2471 - val_loss: 0.2336\n",
      "Epoch 72/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2463 - val_loss: 0.2332\n",
      "Epoch 73/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2442 - val_loss: 0.2330\n",
      "Epoch 74/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2450 - val_loss: 0.2325\n",
      "Epoch 75/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2437 - val_loss: 0.2325\n",
      "Epoch 76/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2428 - val_loss: 0.2319\n",
      "Epoch 77/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2419 - val_loss: 0.2319\n",
      "Epoch 78/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2429 - val_loss: 0.2318\n",
      "Epoch 79/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2440 - val_loss: 0.2314\n",
      "Epoch 80/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2422 - val_loss: 0.2310\n",
      "Epoch 81/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2408 - val_loss: 0.2317\n",
      "Epoch 82/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2409 - val_loss: 0.2311\n",
      "Epoch 83/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2407 - val_loss: 0.2307\n",
      "Epoch 84/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2389 - val_loss: 0.2308\n",
      "Epoch 85/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2398 - val_loss: 0.2302\n",
      "Epoch 86/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2387 - val_loss: 0.2300\n",
      "Epoch 87/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2398 - val_loss: 0.2296\n",
      "Epoch 88/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2384 - val_loss: 0.2300\n",
      "Epoch 89/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2376 - val_loss: 0.2296\n",
      "Epoch 90/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2385 - val_loss: 0.2295\n",
      "Epoch 91/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2360 - val_loss: 0.2293\n",
      "Epoch 92/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2381 - val_loss: 0.2291\n",
      "Epoch 93/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2372 - val_loss: 0.2291\n",
      "Epoch 94/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2354 - val_loss: 0.2297\n",
      "Epoch 95/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2355 - val_loss: 0.2291\n",
      "Epoch 96/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2357 - val_loss: 0.2290\n",
      "Epoch 97/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2353 - val_loss: 0.2287\n",
      "Epoch 98/200\n",
      "24316/24316 [==============================] - 2s - loss: 0.2352 - val_loss: 0.2286\n",
      "Epoch 99/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2354 - val_loss: 0.2281\n",
      "Epoch 100/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2356 - val_loss: 0.2281\n",
      "Epoch 101/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2346 - val_loss: 0.2291\n",
      "Epoch 102/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2347 - val_loss: 0.2276\n",
      "Epoch 103/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2337 - val_loss: 0.2278\n",
      "Epoch 104/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2319 - val_loss: 0.2282\n",
      "Epoch 105/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2335 - val_loss: 0.2275\n",
      "Epoch 106/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2328 - val_loss: 0.2276\n",
      "Epoch 107/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2327 - val_loss: 0.2274\n",
      "Epoch 108/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2335 - val_loss: 0.2273\n",
      "Epoch 109/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2310 - val_loss: 0.2275\n",
      "Epoch 110/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2341 - val_loss: 0.2270\n",
      "Epoch 111/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2325 - val_loss: 0.2270\n",
      "Epoch 112/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2315 - val_loss: 0.2271\n",
      "Epoch 113/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2326 - val_loss: 0.2271\n",
      "Epoch 114/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2308 - val_loss: 0.2272\n",
      "Epoch 115/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2320 - val_loss: 0.2268\n",
      "Epoch 116/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2318 - val_loss: 0.2268\n",
      "Epoch 117/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2314 - val_loss: 0.2269\n",
      "Epoch 118/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2311 - val_loss: 0.2269\n",
      "Epoch 119/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2322 - val_loss: 0.2266\n",
      "Epoch 120/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2304 - val_loss: 0.2268\n",
      "Epoch 121/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2317 - val_loss: 0.2266\n",
      "Epoch 122/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2322 - val_loss: 0.2264\n",
      "Epoch 123/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2305 - val_loss: 0.2273\n",
      "Epoch 124/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2294 - val_loss: 0.2262\n",
      "Epoch 125/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2300 - val_loss: 0.2266\n",
      "Epoch 126/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2289 - val_loss: 0.2261\n",
      "Epoch 127/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2297 - val_loss: 0.2264\n",
      "Epoch 128/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2298 - val_loss: 0.2265\n",
      "Epoch 129/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2285 - val_loss: 0.2260\n",
      "Epoch 130/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2305 - val_loss: 0.2269\n",
      "Epoch 131/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2304 - val_loss: 0.2260\n",
      "Epoch 132/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2288 - val_loss: 0.2263\n",
      "Epoch 133/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2290 - val_loss: 0.2259\n",
      "Epoch 134/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2289 - val_loss: 0.2259\n",
      "Epoch 135/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2288 - val_loss: 0.2260\n",
      "Epoch 136/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2294 - val_loss: 0.2261\n",
      "Epoch 137/200\n",
      "24316/24316 [==============================] - 1s - loss: 0.2277 - val_loss: 0.2260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b0bef0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 3, deeper with dropout\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(256,input_dim=(64),activation='relu'))\n",
    "#model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(256,input_dim=(64),activation='relu'))\n",
    "#model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(128,activation='relu'))\n",
    "model_3.add(Dense(1,activation='linear'))\n",
    "#model_3.compile(loss=rmsle,optimizer='adam')\n",
    "model_3.compile(loss='mean_squared_logarithmic_error',optimizer='adam')\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "    #callbacks.\n",
    "    ModelCheckpoint('keras_checkpoints/model_3', monitor='val_loss', verbose=0, \n",
    "                    save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "]\n",
    "\n",
    "model_3.fit(pca_x_train_2,pca_y_train_2,epochs=200,batch_size=128,validation_data=(pca_x_val,pca_y_val),callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id   price_doc\n",
      "0  30474  3551221.50\n",
      "1  30475  7702081.00\n",
      "2  30476  3903168.75\n",
      "3  30477  4534123.00\n",
      "4  30478  3573680.00\n"
     ]
    }
   ],
   "source": [
    "model_pred_3 = model_3.predict(pca_x_test)\n",
    "\n",
    "output_predictions(\"keras_2_model_3\",id_test,model_pred_3[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24316 samples, validate on 6080 samples\n",
      "Epoch 1/300\n",
      "24316/24316 [==============================] - 3s - loss: 76.8244 - val_loss: 38.8923\n",
      "Epoch 2/300\n",
      "24316/24316 [==============================] - 3s - loss: 29.0525 - val_loss: 21.1462\n",
      "Epoch 3/300\n",
      "24316/24316 [==============================] - 3s - loss: 17.1746 - val_loss: 13.3246\n",
      "Epoch 4/300\n",
      "24316/24316 [==============================] - 3s - loss: 11.2724 - val_loss: 9.0134\n",
      "Epoch 5/300\n",
      "24316/24316 [==============================] - 3s - loss: 7.8549 - val_loss: 6.3463\n",
      "Epoch 6/300\n",
      "24316/24316 [==============================] - 3s - loss: 5.6499 - val_loss: 4.5850\n",
      "Epoch 7/300\n",
      "24316/24316 [==============================] - 3s - loss: 4.1603 - val_loss: 3.3744\n",
      "Epoch 8/300\n",
      "24316/24316 [==============================] - 3s - loss: 3.1119 - val_loss: 2.5202\n",
      "Epoch 9/300\n",
      "24316/24316 [==============================] - 2s - loss: 2.3623 - val_loss: 1.9097\n",
      "Epoch 10/300\n",
      "24316/24316 [==============================] - 2s - loss: 1.8270 - val_loss: 1.4683\n",
      "Epoch 11/300\n",
      "24316/24316 [==============================] - 2s - loss: 1.4320 - val_loss: 1.1481\n",
      "Epoch 12/300\n",
      "24316/24316 [==============================] - 2s - loss: 1.1452 - val_loss: 0.9176\n",
      "Epoch 13/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.9408 - val_loss: 0.7511\n",
      "Epoch 14/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.7942 - val_loss: 0.6321\n",
      "Epoch 15/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.6803 - val_loss: 0.5481\n",
      "Epoch 16/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.6028 - val_loss: 0.4895\n",
      "Epoch 17/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.5441 - val_loss: 0.4490\n",
      "Epoch 18/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.5023 - val_loss: 0.4207\n",
      "Epoch 19/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4720 - val_loss: 0.4011\n",
      "Epoch 20/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4545 - val_loss: 0.3869\n",
      "Epoch 21/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4385 - val_loss: 0.3762\n",
      "Epoch 22/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4266 - val_loss: 0.3676\n",
      "Epoch 23/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4152 - val_loss: 0.3601\n",
      "Epoch 24/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4089 - val_loss: 0.3532\n",
      "Epoch 25/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.4029 - val_loss: 0.3464\n",
      "Epoch 26/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3914 - val_loss: 0.3398\n",
      "Epoch 27/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3884 - val_loss: 0.3334\n",
      "Epoch 28/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3793 - val_loss: 0.3270\n",
      "Epoch 29/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3710 - val_loss: 0.3207\n",
      "Epoch 30/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3656 - val_loss: 0.3146\n",
      "Epoch 31/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3625 - val_loss: 0.3084\n",
      "Epoch 32/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3545 - val_loss: 0.3025\n",
      "Epoch 33/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3479 - val_loss: 0.2966\n",
      "Epoch 34/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3393 - val_loss: 0.2910\n",
      "Epoch 35/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3344 - val_loss: 0.2855\n",
      "Epoch 36/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3272 - val_loss: 0.2801\n",
      "Epoch 37/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3224 - val_loss: 0.2751\n",
      "Epoch 38/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3170 - val_loss: 0.2701\n",
      "Epoch 39/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3108 - val_loss: 0.2657\n",
      "Epoch 40/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3081 - val_loss: 0.2616\n",
      "Epoch 41/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.3013 - val_loss: 0.2576\n",
      "Epoch 42/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2987 - val_loss: 0.2546\n",
      "Epoch 43/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2983 - val_loss: 0.2512\n",
      "Epoch 44/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2917 - val_loss: 0.2487\n",
      "Epoch 45/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2902 - val_loss: 0.2459\n",
      "Epoch 46/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2854 - val_loss: 0.2440\n",
      "Epoch 47/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2832 - val_loss: 0.2416\n",
      "Epoch 48/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2770 - val_loss: 0.2397\n",
      "Epoch 49/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2763 - val_loss: 0.2390\n",
      "Epoch 50/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2755 - val_loss: 0.2374\n",
      "Epoch 51/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2744 - val_loss: 0.2353\n",
      "Epoch 52/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2725 - val_loss: 0.2338\n",
      "Epoch 53/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2697 - val_loss: 0.2328\n",
      "Epoch 54/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2664 - val_loss: 0.2315\n",
      "Epoch 55/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2660 - val_loss: 0.2298\n",
      "Epoch 56/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2624 - val_loss: 0.2292\n",
      "Epoch 57/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2625 - val_loss: 0.2279\n",
      "Epoch 58/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2627 - val_loss: 0.2275\n",
      "Epoch 59/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2593 - val_loss: 0.2266\n",
      "Epoch 60/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2582 - val_loss: 0.2261\n",
      "Epoch 61/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2565 - val_loss: 0.2243\n",
      "Epoch 62/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2554 - val_loss: 0.2249\n",
      "Epoch 63/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2537 - val_loss: 0.2236\n",
      "Epoch 64/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2531 - val_loss: 0.2226\n",
      "Epoch 65/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2522 - val_loss: 0.2228\n",
      "Epoch 66/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2490 - val_loss: 0.2217\n",
      "Epoch 67/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2482 - val_loss: 0.2219\n",
      "Epoch 68/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2474 - val_loss: 0.2204\n",
      "Epoch 69/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2475 - val_loss: 0.2204\n",
      "Epoch 70/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2482 - val_loss: 0.2200\n",
      "Epoch 71/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2441 - val_loss: 0.2194\n",
      "Epoch 72/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2441 - val_loss: 0.2189\n",
      "Epoch 73/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2435 - val_loss: 0.2200\n",
      "Epoch 74/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2446 - val_loss: 0.2198\n",
      "Epoch 75/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2417 - val_loss: 0.2178\n",
      "Epoch 76/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2422 - val_loss: 0.2184\n",
      "Epoch 77/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2404 - val_loss: 0.2175\n",
      "Epoch 78/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2421 - val_loss: 0.2178\n",
      "Epoch 79/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2413 - val_loss: 0.2174\n",
      "Epoch 80/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2385 - val_loss: 0.2177\n",
      "Epoch 81/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2395 - val_loss: 0.2167\n",
      "Epoch 82/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2377 - val_loss: 0.2173\n",
      "Epoch 83/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2377 - val_loss: 0.2168\n",
      "Epoch 84/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2376 - val_loss: 0.2165\n",
      "Epoch 85/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2389 - val_loss: 0.2166\n",
      "Epoch 86/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2346 - val_loss: 0.2173\n",
      "Epoch 87/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2365 - val_loss: 0.2163\n",
      "Epoch 88/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2365 - val_loss: 0.2164\n",
      "Epoch 89/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2346 - val_loss: 0.2163\n",
      "Epoch 90/300\n",
      "24316/24316 [==============================] - 3s - loss: 0.2343 - val_loss: 0.2154\n",
      "Epoch 91/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2333 - val_loss: 0.2165\n",
      "Epoch 92/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2343 - val_loss: 0.2155\n",
      "Epoch 93/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2321 - val_loss: 0.2160\n",
      "Epoch 94/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2329 - val_loss: 0.2148\n",
      "Epoch 95/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2327 - val_loss: 0.2170\n",
      "Epoch 96/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2336 - val_loss: 0.2155\n",
      "Epoch 97/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2327 - val_loss: 0.2159\n",
      "Epoch 98/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2321 - val_loss: 0.2151\n",
      "Epoch 99/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2312 - val_loss: 0.2151\n",
      "Epoch 100/300\n",
      "24316/24316 [==============================] - 2s - loss: 0.2304 - val_loss: 0.2149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x217cea58>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 4 deeper with he_normal weight initializers\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Dense(512,input_dim=(371),activation='relu',kernel_initializer='he_normal'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(256,activation='relu',kernel_initializer='he_normal'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(128,activation='relu',kernel_initializer='he_normal'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(1,activation='linear'))\n",
    "#model_4.compile(loss=rmsle,optimizer='adam')\n",
    "model_4.compile(loss='mean_squared_logarithmic_error',optimizer='adam')\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "    #callbacks.\n",
    "    ModelCheckpoint('keras_checkpoints/model_5', monitor='val_loss', verbose=0, \n",
    "                    save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "]\n",
    "\n",
    "model_4.fit(x_train_2,y_train_2,epochs=300,batch_size=128,validation_data=(x_val,y_val),callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id   price_doc\n",
      "0  30474  4588600.00\n",
      "1  30475  7481042.50\n",
      "2  30476  4112582.75\n",
      "3  30477  6712090.00\n",
      "4  30478  4289107.50\n"
     ]
    }
   ],
   "source": [
    "model_pred_4 = model_4.predict(x_test)\n",
    "\n",
    "output_predictions(\"keras_2_model_6\",id_test,model_pred_4[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### best model was the 3 layer, 128 batch like 500 128 64 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convenience function to add indicator columns for variables with a NA value\n",
    "def add_na_indicator_columns(df):\n",
    "    col_length = len(df)\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().values.any():\n",
    "            temp_series = pd.Series(np.zeros(col_length) )\n",
    "            temp_series[df[i].isnull().values] = 1\n",
    "            new_col_name = 'nan_bool_'+i\n",
    "            df[new_col_name] = pd.Series(temp_series,index=df.index)\n",
    "            #df_test_data['latitude_modified'] = pd.Series(latitude_modified,index=df_test_data.index)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
